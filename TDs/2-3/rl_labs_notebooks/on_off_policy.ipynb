{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding on-policy and off-policy properties \n",
    "\n",
    "In this part, we investigate the difference between the off-policy property of Q-learning and the on-policy property of SARSA. For doing so, we will feed the critic of these algorithms with off-policy data stored into a replay buffer.\n",
    "Using a replay buffer is not mandatory when using random uniform samples, but using this approach makes the architecture more flexible for later experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class Sample\n",
    "\n",
    "A sample contains the following information : the current state, the current action, the resulting reward, and the resulting next state.\n",
    "\n",
    "In the cell below, create a class to store a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"define your Sample class here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class ReplayBuffer\n",
    "\n",
    "A replay buffer is a FIFO list of samples with a limited size (often set to 10^6). \n",
    "\n",
    "In the cell below, create a class ReplayBuffer. Give it a method __add(sample)__ to add a sample, a method __draw()__ to draw a sample randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"define your ReplayBuffer class here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your mdp class, add a method __uniformSampling()__ which returns a randomly generated sample: it draws a state and an action at random, provides the corresponding reward and next state, stores these into a sample and returns that sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the box below, get your Q-learning code from [reinforcement_learning.ipynb](reinforcement_learning.ipynb), and copy-paste it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"copy-paste your Q-learning code here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the above code so that, instead of being fed by the samples of an agent running its current policy, the Q-table is fed by samples drawn randomly from the replay buffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the main code\n",
    " \n",
    " Below, write a code that:\n",
    " * creates a replay buffer containing 10.000 samples drawn uniformly from the mdp,\n",
    " * makes Q-learning learn from samples drawn uniformly from the replay buffer for a fixed number of episodes,\n",
    " * visualizes the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"run modified Qlearning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"visualize your results here, it is better to do it in a different cell than the one that runs your algorithm so\"\n",
    "\"you can customize your rendering to your heart's content !\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Is this algorithm converging to an optimal Q-table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, do the same with SARSA: import it below, and modify it as you did for Q-learning (or just change your modified Q-learning into a modified SARSA below, this is easier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"write your SARSA code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"run your modified SARSA code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"visualize your results here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Again, what do you observe? Is this algorithm converging to an optimal Q-table?\n",
    "Discuss the difference.\n",
    "To go further and better understand the convergence properties of SARSA, read [the paper where its convergence was proven](https://link.springer.com/content/pdf/10.1023/A:1007678930559.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
