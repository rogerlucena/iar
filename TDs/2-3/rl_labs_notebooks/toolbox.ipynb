{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a few functions used accross the labs, as well as the constants for the possible actions in the maze. \n",
    "\n",
    "### Action selection\n",
    "\n",
    "At each step of a reinforcement learning episode, the agent has to select its next action. In order to do this, it usually computes a Q table of state-action values. When *exploiting*, it takes the best action in the current state according to the Q table. This is the greedy method. When *exploring*, it has to do something else to avoid being stuck in a local optimum. This is why other action selection methods exist, like $\\epsilon$-greedy and softmax. \n",
    "\n",
    "When following an $\\epsilon$-greedy behaviour, the agent selects the action with the highest state-action value most of the time (exploitation) and, with probability $\\epsilon$, it performs an action drawn randomly, uniformly and independently from the state-action values estimates (exploration).\n",
    "\n",
    "One drawback of the $\\epsilon$-greedy selection is that the state values do not matter when exploring, as the agent chooses equally among all actions. The softmax rule solves this problem by setting the probability of each action according to the Q table. There are many softmax rules, the most used one relies on Gibbs distribution:\n",
    "$$\\displaystyle \\pi^{(t)}(u|x) = \\frac {\\exp(\\frac{Q^{(t)}(x,u)}{\\tau})}{\\sum_{v \\in V}\\exp(\\frac{Q^{(t)}(x,v)}{\\tau})},$$\n",
    "\n",
    "where $\\tau$ is a positive parameter called temperature. The higher the temperature, the closer the choice to equiprobability.\n",
    "\n",
    "The assignments of the labs rely on the softmax rule defined below. You can add a code for the $\\epsilon$-greedy method and use it instead of the softmax, then analyze your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "##########-action constants-###################\n",
    "N = 0\n",
    "S = 1\n",
    "E = 2\n",
    "W = 3\n",
    "NOOP = 4  \n",
    "\n",
    "\n",
    "def discreteProb(p):\n",
    "        # Draw a random number using probability table p (column vector)\n",
    "        # Suppose probabilities p=[p(1) ... p(n)] for the values [1:n] are given, sum(p)=1 \n",
    "        # and the components p(j) are nonnegative. \n",
    "        # To generate a random sample of size m from this distribution,\n",
    "        #imagine that the interval (0,1) is divided into intervals with the lengths p(1),...,p(n). \n",
    "        # Generate a uniform number rand, if this number falls in the jth interval given the discrete distribution,\n",
    "        # return the value j. Repeat m times.\n",
    "        r = np.random.random()\n",
    "        cumprob=np.hstack((np.zeros(1),p.cumsum()))\n",
    "        sample = -1\n",
    "        for j in range(p.size):\n",
    "            if (r>cumprob[j]) & (r<=cumprob[j+1]):\n",
    "                sample = j\n",
    "                break\n",
    "        return sample\n",
    "\n",
    "def softmax(Q,x,tau):\n",
    "    # Returns a soft-max probability distribution over actions\n",
    "    # Inputs :\n",
    "    # - Q : a Q-function represented as a nX times nU matrix\n",
    "    # - x : the state for which we want the soft-max distribution\n",
    "    # - tau : temperature parameter of the soft-max distribution\n",
    "    # Output :\n",
    "    # - p : probability of each action according to the soft-max distribution\n",
    "    \n",
    "    p = np.zeros((len(Q[x])))\n",
    "    sump = 0\n",
    "    for i in range(len(p)) :\n",
    "        p[i] = np.exp((Q[x,i]/tau).round(5))\n",
    "        sump += p[i]\n",
    "    \n",
    "    p = p/sump\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "def compare(V,Q,pol): \n",
    "    # compares the state value V with the state-action value Q following policy pol\n",
    "    epsilon = 0.01 # precision of the comparison\n",
    "    sumval = np.zeros((V.size))\n",
    "    for i in range(V.size): # compute the difference between V and Q for each state\n",
    "        sumval[i] = abs(V[i] - Q[i,pol[i]])\n",
    "         \n",
    "    if np.max(sumval)<epsilon :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
