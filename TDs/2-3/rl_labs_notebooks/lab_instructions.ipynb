{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "This is a set of basic reinforcement learning (RL) labs based on jupyter notebooks. The idea is to convey a set of basic reinforcement learning concepts through coding in python and experimentation with graphical displays. \n",
    "\n",
    "The labs are intended for being available to anyone through internet, so the assignments can be followed in full autonomy. If your are following these labs on your own through internet, you can obviously skip whatever you are not interested into, but grossly following the order of exercises below is strongly suggested as some concepts and skills are introduced incrementally. If your are following these labs in the context of a class, well, just do whatever the teacher is asking ;)\n",
    "\n",
    "If you want to follow these labs on your own and you don't have a background in RL yet, you may first have a look at my [videos and slides](http://pages.isir.upmc.fr/~sigaud/teach/english.html), which are themselves inspired from the excellent book: \"Reinforcement Learning, an introduction\", Richard R. Sutton and Andrew G. Barto (1998), The MIT press. \n",
    "A newer edition of this book is available [here](http://incompleteideas.net/book/bookdraft2018jan1.pdf).\n",
    "\n",
    "Note that the visualization of the simulations all come with a few buttons at the bottom, but these buttons cannot be used. This will be fixed some day.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 1. MDPs and mazes\n",
    "\n",
    "A reinforcement learning agent interacts with an environment represented as an MDP (Markov Decision Process). It is defined by a tuple $(S, A, P, r, \\gamma)$ where $S$ is the state space, $A$ the action space, $P(state_t, action_t, state_{t+1})$ the transition function, $r(state_t, action_t)$ the reward function and $\\gamma \\in [0, 1]$ the discount factor.\n",
    "\n",
    "The maze_mdp() and mdp() classes are defined in the [mdp.ipynb](mdp.ipynb) notebook. \n",
    "Though it is not crucial to understand them for completing these labs, you may have a look (**OPTIONAL**):\n",
    "\n",
    "* open the [mdp.ipynb](mdp.ipynb) notebook, have a look at how MDPs and mazes are defined\n",
    "* in the last cell of the notebook, modify the code so as to get a maze corresponding to the figure below:\n",
    "\n",
    "![sample_maze|400x300](sample_maze.png)\n",
    "\n",
    "\n",
    "* copy the mdp.ipynb notebook into my_mdp.ipynb\n",
    "* modify the mdp so that actions have a stochastic outcome: each time the agent chooses a direction, there is 85% chance that it performs the expected movement, and a 5% chance per direction that it moves in one of the three other directions.\n",
    "* later, when running your dynamic programming and reinforcement learning functions, evaluate them both on mdp.ipynb and my_mdp.ipynb, and see if there is a difference in convergence time and properties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dynamic Programming\n",
    "\n",
    "Dynamic programming algorithms are designed to work in an MDP which is given to the agent. They find the optimal policy by computing a value function V or an action-value function Q over the state space or state-action space of the given MDP. Value iteration and policy iteration are two standard dynamic programming algorithms which work in a quite different way. You should study both of them using both the value and the action-value function, as these algorithms contain the basic building blocks for Q-learning, SARSA and actor-critic algorithms, as well as model-based methods.\n",
    "\n",
    "To do the corresponding lab, open the [dynamic_programming.ipynb](dynamic_programming.ipynb) notebook and follow instructions therein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. (Model-Free) Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is about finding the optimal policy in an MDP when the MDP is initially unknown to the agent. Generally speaking, the agent has to *explore* the MDP to figure out which action in which state leads to which other state and reward. The model-free case is about finding this optimal policy just through very local updates, without storing any information about previous interactions with the environment. Principles of these local updates can already be found in the Temporal Difference (TD) algorithm, which iteratively computes optimal values for all state using local updates.\n",
    "\n",
    "The most widely used model-free RL algorithms are Q-learning, SARSA and actor-critic algorithms.\n",
    "\n",
    "To study these algorithms, open the [reinforcement_learning.ipynb](reinforcement_learning.ipynb) notebook and follow instructions therein.\n",
    "\n",
    "Note that studying actor-critic algorithms is a must if you want to study deep RL afterwards, because several state-of-the-art deep RL algorithms are actor-critic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Model-Based Reinforcement Learning\n",
    "\n",
    "The model-based RL (MBRL) approach is conceptually simpler than its model-free counterpart. To obtain an optimal policy in an MDP that you don't know, you may explore this MDP while building a model of its transition and its reward function, and then apply dynamic programming on this model. Actually, you can learn the model simultaneously with applying dynamic programming, but this may raise convergence issue.\n",
    "\n",
    "By opening [mbrl.ipynb](mbrl.ipynb) and following instructions therein, you will study the Real-Time Dynamic Programming (RTDP) algorithm, one standard instance of MBRL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. On-policy and Off-policy, with SARSA and Q-learning\n",
    "\n",
    "An important difference between Q-learning and SARSA is that the former is off-policy while the latter is on-policy. To understand what it means in practice, you will investigate their behavior when they are trained using data coming from a random, uniform sampling policy. \n",
    "\n",
    "Open the [on_off_policy.ipynb](on_off_policy.ipynb) notebook and follow instructions therein.\n",
    "\n",
    "\n",
    "Getting these concepts is important if you want to study deep RL afterwards, because the on-policy versus off-policy distinction plays a critical role in this literature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. N-step Q-learning\n",
    "\n",
    "A drawback of temporal difference algorithms like SARSA and Q-learning is that they can be very slow at propagating values over states. There are several fixes to this issue: using model-based RL, using eligibility traces, or more simply using N-step return.\n",
    "\n",
    "By opening [nsteps.ipynb](nsteps.ipynb), you will learn how to do the latter. Again, this is an important topic to understand several state-of-the-art deep RL algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Actor-Critic\n",
    "\n",
    "The Actor-Critic algorithm is a model-free RL algorithm which simultaneously manages an estimate of the value (or action-value, or advantage) function, called the critic, and a policy, called the actor. Though it is not the simplest to explain and code, this algorithm was invented very early in the RL history. It has good properties in the continuous action domain, hence it played a key role in the advent of deep RL with continuous actions.\n",
    "\n",
    "To implement an Actor-Critic algorithm, open the [actor_critic.ipynb](actor_critic.ipynb) notebook and follow instructions therein."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
