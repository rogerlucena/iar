{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model-Based Reinforcement Learning (MBRL)\n",
    "\n",
    "We are now going to use Model-Based Reinforcement Learning (MBRL) techniques to solve our MDP. In the Model-Based approach, we learn a model of the transition and the reward function and apply Dynamic Programming (DP) techniques using this model instead of the true MDP.\n",
    "\n",
    "\n",
    "## Real-Time Dynamic Programming (RTDP)\n",
    "\n",
    "Let $\\hat{P}^{(t)}$ denote the estimate of the transition probabilities at time $t$ (i.e., after the robot has experienced $t$ transitions and performed as many updates). The estimate $\\hat{P}^{(t)}$ can then be updated as\n",
    "\n",
    "$ \\displaystyle \\hat{P}^{(t+1)}(x_t, u_t, y) = (1 - \\frac{1}{N_t(x_t, u_t)})\\hat{P}^{(t)}(x_t, u_t, y) + \\frac{1}{N_t(x_t, u_t)} \\delta_y(x_{t+1})$\n",
    "\n",
    "where $\\delta_y(x) = 1$ if $y = x$ and 0 otherwise, and $N_t(x, u)$ denotes the number of visits to the (x, u) pair\n",
    "up to and including time $t$. Note that, at time $t$, only the transition probabilities $\\hat{P}^{(t)}(x_t, u_t, .)$\n",
    "are updated, to reflect the new transition just observed. For the reward function estimate $\\hat{r}$, we\n",
    "simply have $\\hat{r}^{(t+1)}(x_t, u_t) = r^{(t)}$.\n",
    "\n",
    "Therefore, we don't have to estimate the reward function explicitly, i.e. we can use the reward samples instead.\n",
    "\n",
    "Once $\\hat{P}$ and $\\hat{r}$ are properly estimated, we can now use them in a Value Iteration scheme.\n",
    "\n",
    "An interesting aspect of the model-based approaches, however, is that these iterations can be\n",
    "performed simultaneously with $\\hat{P}$ and $\\hat{r}$ updates. \n",
    "\n",
    "This technique is called Real-Time Dynamic Programming.\n",
    "\n",
    "The code is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "################################ Model-Based Reinforcement Learning ################################\n",
    "\n",
    "# Learns a model of the transition function hatP and reward function hatR\n",
    "\n",
    "########################### Real-Time Dynamic Programming ###########################\n",
    "\n",
    "\n",
    "\n",
    "def RTDP(mdp, deviation=0, nEpisodes=10000, nTimesteps=50, render=True):  # Performs an estimation of the transition function hatP only (no reward estimation)\n",
    "    # Deviation param is used to add some noise to the reward function      \n",
    "    \n",
    "    # Initialize state-action value function to zero\n",
    "    Q = np.zeros((mdp.observation_space.size,mdp.action_space.size)) \n",
    "    \n",
    "    # Initialize state function to a uniform distribution\n",
    "    hatP = np.ones((mdp.observation_space.size+1,mdp.action_space.size,mdp.observation_space.size+1))/mdp.observation_space.size \n",
    "    \n",
    "    # Initialize state-action count to one\n",
    "    N = np.ones((mdp.observation_space.size,mdp.action_space.size))\n",
    "    \n",
    "    \n",
    "    # Initialize Qmax, state value and policy lists\n",
    "    Qmax = Q.max(axis=1)\n",
    "    Q_list = [Q.copy()]\n",
    "    policy_list = [np.argmax(Q,axis=1)]\n",
    "    \n",
    "    if render:\n",
    "        mdp.new_render()\n",
    "    \n",
    "    mdp.timeout = nTimesteps\n",
    "    \n",
    "    for i in range(nEpisodes) : # For each episode\n",
    "        \n",
    "        # Draw an initial state (set uniform to False to sample according to P0 distribution)\n",
    "        x = mdp.reset(uniform=True)\n",
    "        done = mdp.done()\n",
    "        while not done: \n",
    "            if render :\n",
    "                    mdp.render(Q,Q.argmax(axis=1))\n",
    "            \n",
    "            # Draw an action randomly\n",
    "            u = mdp.action_space.sample()\n",
    "            \n",
    "            # Perform a step forward in the MDP using action u\n",
    "            [y,r,done,info] = mdp.step(u,deviation=deviation)\n",
    "\n",
    "            # Update the transition function estimation hatP\n",
    "            for k in mdp.observation_space.states :\n",
    "                hatP[x,u,k] = (1-1/N[x,u])*(hatP[x,u,k])\n",
    "            hatP[x,u,y] += 1/N[x,u]\n",
    "            \n",
    "            # Update state-action table based on the new hatP\n",
    "            if x in mdp.terminal_states:\n",
    "                Q[x,u] = r\n",
    "            else:\n",
    "                Q[x,u] = r + mdp.gamma*np.sum([hatP[x,u,k]*Qmax[k] for k in mdp.observation_space.states])\n",
    "            Qmax = np.max(Q,axis=1)\n",
    "        \n",
    "            # Increment the counter\n",
    "            N[x,u] = N[x,u]+1      \n",
    "            \n",
    "            # Update agent's position\n",
    "            x = y\n",
    "\n",
    "        # Save computed state-action value Q and policy\n",
    "        Q_list.append(Q.copy())\n",
    "        policy_list.append(Q.argmax(axis=1))\n",
    "        \n",
    "    if render :\n",
    "        mdp.render(Q,Q.argmax(axis=1))\n",
    "            \n",
    "            \n",
    "    return [Q_list,policy_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, write the code to run RTDP. How does it compare with Q-learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"write the code to run RTDP here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code to visualize the RTDP process after having run your code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize your results\n",
    "m.new_render()\n",
    "m.render(Q_list[-1], policy_list[-1])\n",
    "\n",
    "#nb_frames = 100\n",
    "#ani = m.create_animation(Q_list, policy_list, nb_frames) \n",
    "#HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RTPD for stochastic rewards\n",
    "\n",
    "The code above only deals with deterministic rewards.\n",
    "\n",
    "Modify the classes in [mdp.ipynb](mdp.ipynb) so as to add a Gaussian noise of standard deviation 0.1 to the reward (see the function np.random.randn()). What happens if you set the standard deviation to 1.0? \n",
    "\n",
    "Copy-paste the RTDP code above in the cell below, and update it so as to deal with stochastic rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"write the stochastic reward RTDP code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"run it\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code to visualize the RTDP process after having run your code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize your results\n",
    "%matplotlib notebook\n",
    "\n",
    "from ipynb.fs.defs.maze_plotter import maze_plotter # used to generate a video describing the state value and policy evolution\n",
    "from IPython.display import HTML # used to display the video inside the notebook\n",
    "\n",
    "nb_frames = 50\n",
    "ani = m.create_animation(Q_list, policy_list, nb_frames)\n",
    "#HTML(ani.to_jshtml()) # loads your video and enables a video widget\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
